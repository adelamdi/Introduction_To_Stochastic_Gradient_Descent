{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Stochastic Gradient Descent\n",
        "## May, 2024\n",
        "## A hands-on notebook by Fariman.AA and Kiani.M\n"
      ],
      "metadata": {
        "id": "23JgblB8dLcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction, Whats gradient descent algorithm?\n"
      ],
      "metadata": {
        "id": "K_H4as75eTkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stochastic gradient descent (SGD)"
      ],
      "metadata": {
        "id": "BTC4RqPTcyXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why we use Stochastic gradient descent (SGD)?"
      ],
      "metadata": {
        "id": "8RI7fYlt8oT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stochastic gradient descent algorithm implementation in Python\n",
        "\n"
      ],
      "metadata": {
        "id": "EitDoY3K9Q6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section of the notebook, we present an implementation of the Stochastic Gradient Descent (SGD) algorithm in Python. The implementation leverages the NumPy library, so first we import NumPy."
      ],
      "metadata": {
        "id": "IqDp--mz8gim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "zx2UDFw579YK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define few hyperparameters that influence the SGD optimization process.\n",
        "\n",
        "* learning_rate: This parameter controls the step size taken in each iteration.\n",
        "* epochs: This parameter specifies the number iterations.\n",
        "* batch_size: This parameter determines the number of data samples used for updating values in each iteration.\n",
        "* tolerance: This parameter defines the convergence criterion .\n",
        "* weights: This variable represents the weight associated with the model $ (y = weight * x + bias) $.\n",
        "* bias: This variable represents the bias associated with the model $ (y = weight * x + bias) $ ."
      ],
      "metadata": {
        "id": "6gpT9uPs9raW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "batch_size = 32\n",
        "tolerance = 1e-3\n",
        "weights = None\n",
        "bias = None"
      ],
      "metadata": {
        "id": "gEIoeeyS67Jq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We proceed by defining a predict function. This function takes an input vector, denoted by X, and returns a predicted output value, denoted by Y. The prediction is computed using the linear regression equation Y = w * X + b, where w represents the weight vector and b represents the bias."
      ],
      "metadata": {
        "id": "5_AUO0ZrHWyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear regression equation, y = w * x + b\n",
        "def calculate(X):\n",
        "  return np.dot(X,weights) + bias"
      ],
      "metadata": {
        "id": "AgzwH7Xg7T0y"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now define the cost function, a significant advantage of SGD lies in its flexibility regarding the choice of cost function. The code can adapt to various cost functions. In this instance, we have opted for the Mean Squared Error (MSE) cost function."
      ],
      "metadata": {
        "id": "YypBQSPyH1f3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cost function\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)"
      ],
      "metadata": {
        "id": "U6wKJzTH7a9K"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets define our gradient function. The gradient function calculates the gradients of the cost function (which we'll assume is MSE in this case) with respect to the weights (w) and the bias (b) for a given batch of data (X_batch, y_batch)."
      ],
      "metadata": {
        "id": "QMyEGpqNIV_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Return gradient_weights (partial derivative with respect to weights) and\n",
        "# griadient_bias (partial derivative with respect to bias)\n",
        "def gradient(X_batch, y_batch):\n",
        "    y_pred = calculate(X_batch)\n",
        "    error = y_pred - y_batch\n",
        "    gradient_weights = np.dot(X_batch.T, error) / X_batch.shape[0]\n",
        "    # we dont use mean_square_error function here for calculatin gradient bias\n",
        "    # becuse we have error defiend earlier for returning gradient_weights\n",
        "    gradient_bias = np.mean(error)\n",
        "    return gradient_weights, gradient_bias"
      ],
      "metadata": {
        "id": "RdKVrFCS7evC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now introduce the SGD function, this function follows an structure:\n",
        "* Initialization: The function begins by initializing the weights (w) and bias term (b) with random values.\n",
        "* Iteration: The core of the SGD function lies within the iterative loop.\n",
        "* Random batch selection: Within each iteration, a random subset of data points (N) is selected from the dataset.\n",
        "\n",
        "* Gradient update: This function is utilized to calculate the gradients of the cost function with respect to the weights and bias term for the current batch.\n",
        "\n",
        "* Weight and Bias Update: This function updates the weights and bias term according cost function minimization.\n",
        "\n",
        "* Termination: The iteration loop terminates when the maximum number of epochs has been reached or convergence criterion is satisfied.\n"
      ],
      "metadata": {
        "id": "XIRdJKXJJD4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SGD(X, y):\n",
        "    n_samples, n_features = X.shape\n",
        "    global weights\n",
        "    weights = np.random.randn(n_features)\n",
        "    global bias\n",
        "    bias = np.random.randn()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      # np.random.permutation randomly permute a sequence\n",
        "      indices = np.random.permutation(n_samples)\n",
        "      X_shuffled = X[indices]\n",
        "      y_shuffled = y[indices]\n",
        "\n",
        "      for i in range(0, n_samples, batch_size):\n",
        "          X_batch = X_shuffled[i:i+batch_size]\n",
        "          y_batch = y_shuffled[i:i+batch_size]\n",
        "\n",
        "          gradient_weights, gradient_bias = gradient(X_batch, y_batch)\n",
        "          weights -= learning_rate * gradient_weights\n",
        "          bias -= learning_rate * gradient_bias\n",
        "\n",
        "      if epoch % 100 == 0:\n",
        "        y_pred = calculate(X)\n",
        "        loss = mean_squared_error(y, y_pred)\n",
        "        print(f\"Epoch {epoch}: Loss {loss}\")\n",
        "\n",
        "      if np.linalg.norm(gradient_weights) < tolerance:\n",
        "          print(\"Convergence reached.\")\n",
        "          break\n",
        "\n",
        "    return weights, bias"
      ],
      "metadata": {
        "id": "Fzh3rszg7iM_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lets review a simple example of SGD"
      ],
      "metadata": {
        "id": "4ZfepEsD7z6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create random dataset with 100 rows and 5 columns\n",
        "X = np.random.randn(100, 5)\n",
        "# create corresponding target value by adding random\n",
        "# noise in the dataset\n",
        "y = np.dot(X, np.array([1, 2, 3, 4, 5]))\\\n",
        "    + np.random.randn(100) * 0.1\n",
        "\n",
        "w,b = SGD(X,y)\n",
        "print('Best Weight:' + str(w))\n",
        "print('Best Bias:' + str(b))\n",
        "# Predict using predict method from model\n",
        "y_pred = w*X+b\n",
        "#y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g048e6ST3_YF",
        "outputId": "66ee197f-857f-4b4d-8e52-39e5efca901e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Weight:[1.01221837 1.99274418 2.98478503 3.98803252 4.98904489]\n",
            "Best Bias:-0.0009756412032080085\n"
          ]
        }
      ]
    }
  ]
}