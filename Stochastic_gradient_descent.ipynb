{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23JgblB8dLcp"
   },
   "source": [
    "# Introduction to Stochastic Gradient Descent\n",
    "## May, 2024\n",
    "## A hands-on notebook by Fariman.AA and Kiani.M\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_H4as75eTkS"
   },
   "source": [
    "### Introduction, Whats gradient descent algorithm?\n",
    "\n",
    "Gradient descent, a cornerstone of optimization in machine learning, is an iterative technique for finding a function's minimum. \n",
    "\n",
    "1. **Objective Function:** We define a function that quantifies a model's performance, often the discrepancy between its predictions and true values. Minimizing this function improves the model.\n",
    "\n",
    "2. **Gradient:** This mathematical concept points uphill, indicating the direction of steepest ascent at a specific point.\n",
    "\n",
    "3. **Iterative Updates:** We begin with an initial guess. The algorithm then iteratively takes small steps in the opposite direction of the gradient, effectively moving downhill. By continuously traversing this path, we gradually approach the function's minimum.\n",
    "\n",
    "**Types of Gradient Descent:**\n",
    "\n",
    "* **Vanilla Gradient Descent:** The workhorse, it uses the entire dataset to calculate the gradient for each update.\n",
    "\n",
    "* **Stochastic Gradient Descent (SGD):** To handle large datasets efficiently, SGD utilizes a mini-batch of data points for faster updates, introducing a stochastic (random) element.\n",
    "\n",
    "In essence, gradient descent provides a powerful and intuitive way to optimize various machine learning models, guiding them towards the optimal solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTC4RqPTcyXB"
   },
   "source": [
    "### Stochastic gradient descent (SGD)\n",
    "\n",
    "Stochastic gradient descent (SGD) stands as a ubiquitous optimization algorithm within the realm of machine learning. It serves as a variant of the traditional gradient descent algorithm, specifically designed to tackle the challenges associated with large datasets.\n",
    "\n",
    "Conventional gradient descent utilizes the entirety of the training data to compute the gradient at each iteration. While effective for smaller datasets, this approach becomes computationally expensive and inefficient for massive datasets.\n",
    "\n",
    "Stochastic gradient descent stands as a powerful and versatile optimization algorithm, particularly adept at handling large datasets. Its computational efficiency, scalability, and inherent regularization properties make it a go-to choice for a wide range of machine learning tasks. However, it is crucial to acknowledge the potential for convergence issues and suboptimal solutions arising from its stochastic nature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RI7fYlt8oT9"
   },
   "source": [
    "### Why we use Stochastic gradient descent (SGD)?\n",
    "\n",
    "While gradient descent is a powerful optimization technique, it stumbles when dealing with massive datasets. Stochastic gradient descent (SGD) emerges as a solution, offering several compelling advantages:\n",
    "\n",
    "1. **Computational Efficiency:** Gradient descent calculates the update direction using the entire dataset in each iteration. For large datasets, this becomes computationally expensive. SGD utilizes mini-batches, a small subset of data points chosen randomly, to approximate the gradient. This significantly reduces the computational burden per iteration, making SGD ideal for large-scale learning problems.\n",
    "\n",
    "2. **Scalability:** As datasets continue to grow in size and complexity, SGD gracefully adapts. Its reliance on mini-batches ensures efficient optimization even for exceptionally large datasets, making it highly scalable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EitDoY3K9Q6b"
   },
   "source": [
    "### Stochastic gradient descent algorithm implementation in Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqDp--mz8gim"
   },
   "source": [
    "In this section of the notebook, we present an implementation of the Stochastic Gradient Descent (SGD) algorithm in Python. The implementation leverages the NumPy library, so first we import NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zx2UDFw579YK"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gpT9uPs9raW"
   },
   "source": [
    "We define few hyperparameters that influence the SGD optimization process.\n",
    "\n",
    "* learning_rate: This parameter controls the step size taken in each iteration.\n",
    "* epochs: This parameter specifies the number iterations.\n",
    "* batch_size: This parameter determines the number of data samples used for updating values in each iteration.\n",
    "* tolerance: This parameter defines the convergence criterion .\n",
    "* weights: This variable represents the weight associated with the model $ (y = weight * x + bias) $.\n",
    "* bias: This variable represents the bias associated with the model $ (y = weight * x + bias) $ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "gEIoeeyS67Jq"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "tolerance = 1e-3\n",
    "weights = None\n",
    "bias = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_AUO0ZrHWyZ"
   },
   "source": [
    "We proceed by defining a predict function. This function takes an input vector, denoted by X, and returns a predicted output value, denoted by Y. The prediction is computed using the linear regression equation Y = w * X + b, where w represents the weight vector and b represents the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "AgzwH7Xg7T0y"
   },
   "outputs": [],
   "source": [
    "# Linear regression equation, y = w * x + b\n",
    "def calculate(X):\n",
    "  return np.dot(X,weights) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YypBQSPyH1f3"
   },
   "source": [
    "We now define the cost function, a significant advantage of SGD lies in its flexibility regarding the choice of cost function. The code can adapt to various cost functions. In this instance, we have opted for the Mean Squared Error (MSE) cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "U6wKJzTH7a9K"
   },
   "outputs": [],
   "source": [
    "# Cost function\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMyEGpqNIV_4"
   },
   "source": [
    "Lets define our gradient function. The gradient function calculates the gradients of the cost function (which we'll assume is MSE in this case) with respect to the weights (w) and the bias (b) for a given batch of data (X_batch, y_batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "RdKVrFCS7evC"
   },
   "outputs": [],
   "source": [
    "# Return gradient_weights (partial derivative with respect to weights) and\n",
    "# griadient_bias (partial derivative with respect to bias)\n",
    "def gradient(X_batch, y_batch):\n",
    "    y_pred = calculate(X_batch)\n",
    "    error = y_pred - y_batch\n",
    "    gradient_weights = np.dot(X_batch.T, error) / X_batch.shape[0]\n",
    "    # we dont use mean_square_error function here for calculatin gradient bias\n",
    "    # becuse we have error defiend earlier for returning gradient_weights\n",
    "    gradient_bias = np.mean(error)\n",
    "    return gradient_weights, gradient_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIRdJKXJJD4n"
   },
   "source": [
    "We now introduce the SGD function, this function follows an structure:\n",
    "* Initialization: The function begins by initializing the weights (w) and bias term (b) with random values.\n",
    "* Iteration: The core of the SGD function lies within the iterative loop.\n",
    "* Random batch selection: Within each iteration, a random subset of data points (N) is selected from the dataset.\n",
    "\n",
    "* Gradient update: This function is utilized to calculate the gradients of the cost function with respect to the weights and bias term for the current batch.\n",
    "\n",
    "* Weight and Bias Update: This function updates the weights and bias term according cost function minimization.\n",
    "\n",
    "* Termination: The iteration loop terminates when the maximum number of epochs has been reached or convergence criterion is satisfied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Fzh3rszg7iM_"
   },
   "outputs": [],
   "source": [
    "def SGD(X, y):\n",
    "    n_samples, n_features = X.shape\n",
    "    global weights\n",
    "    weights = np.random.randn(n_features)\n",
    "    global bias\n",
    "    bias = np.random.randn()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      # np.random.permutation randomly permute a sequence\n",
    "      indices = np.random.permutation(n_samples)\n",
    "      X_shuffled = X[indices]\n",
    "      y_shuffled = y[indices]\n",
    "\n",
    "      for i in range(0, n_samples, batch_size):\n",
    "          X_batch = X_shuffled[i:i+batch_size]\n",
    "          y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "          gradient_weights, gradient_bias = gradient(X_batch, y_batch)\n",
    "          weights -= learning_rate * gradient_weights\n",
    "          bias -= learning_rate * gradient_bias\n",
    "\n",
    "      if epoch % 100 == 0:\n",
    "        y_pred = calculate(X)\n",
    "        loss = mean_squared_error(y, y_pred)\n",
    "        print(f\"Epoch {epoch}: Loss {loss}\")\n",
    "\n",
    "      if np.linalg.norm(gradient_weights) < tolerance:\n",
    "          print(\"Convergence reached.\")\n",
    "          break\n",
    "\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZfepEsD7z6n"
   },
   "source": [
    "### Lets review a simple example of SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g048e6ST3_YF",
    "outputId": "66ee197f-857f-4b4d-8e52-39e5efca901e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Weight:[1.01221837 1.99274418 2.98478503 3.98803252 4.98904489]\n",
      "Best Bias:-0.0009756412032080085\n"
     ]
    }
   ],
   "source": [
    "# Create random dataset with 100 rows and 5 columns\n",
    "X = np.random.randn(100, 5)\n",
    "# create corresponding target value by adding random\n",
    "# noise in the dataset\n",
    "y = np.dot(X, np.array([1, 2, 3, 4, 5]))\\\n",
    "    + np.random.randn(100) * 0.1\n",
    "\n",
    "w,b = SGD(X,y)\n",
    "print('Best Weight:' + str(w))\n",
    "print('Best Bias:' + str(b))\n",
    "# Predict using predict method from model\n",
    "y_pred = w*X+b\n",
    "#y_pred"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
